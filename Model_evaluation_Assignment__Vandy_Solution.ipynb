{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vandana485/Vandy/blob/master/Model_evaluation_Assignment__Vandy_Solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1817658",
      "metadata": {
        "id": "e1817658"
      },
      "source": [
        "<h2 style=\"color: green; text-align: center; font-weight: bold;\">Post class assignment</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction"
      ],
      "metadata": {
        "id": "R2bED8wpDbjl"
      },
      "id": "R2bED8wpDbjl"
    },
    {
      "cell_type": "markdown",
      "id": "c5629aa3",
      "metadata": {
        "id": "c5629aa3"
      },
      "source": [
        "#### XYZ Bank - Term Deposit Subscription Prediction\n",
        "* You are a data scientist at XYZ Bank, tasked with helping the bank increase the success rate of its term deposit campaigns. In the previous campaigns, the bank noticed that only a small fraction of customers subscribed to term deposits, and now they want to use data science to predict which customers are likely to subscribe.\n",
        "\n",
        "<center><img src=\"https://dcbtehri.co.in/wp-content/uploads/2020/05/FD1.jpg\"/></center>\n",
        "\n",
        "* Youâ€™ve been provided with a dataset that includes information about customer demographics, financial behavior, and previous campaign contact history. Your task is to evaluate different machine learning models and help the bank efficiently target potential customers for their term deposit marketing efforts."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Import the neccessary libraries"
      ],
      "metadata": {
        "id": "3X2_Rb1pD5gO"
      },
      "id": "3X2_Rb1pD5gO"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9531e156",
      "metadata": {
        "id": "9531e156"
      },
      "outputs": [],
      "source": [
        "# Import the necessary libraries(numpy, pandas, matplotlib, seaborn)\n",
        "# Data manipulation and numerical operations\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Setting maximum limit of the number of columns visible\n",
        "pd.set_option('display.max_columns', 30)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Load the data"
      ],
      "metadata": {
        "id": "3hvU39A8Ebdo"
      },
      "id": "3hvU39A8Ebdo"
    },
    {
      "cell_type": "markdown",
      "id": "c707f440",
      "metadata": {
        "id": "c707f440"
      },
      "source": [
        "\n",
        "### The dataset contains 16 features and a target as shown below:\n",
        "1. `age`: The customer's age.\n",
        "2. `job`: The type of job the customer has.\n",
        "3. `marital`: The marital status of the customer.\n",
        "4. `education`: The customer's level of education.\n",
        "5. `default`: Whether the customer has credit in default.\n",
        "6. `balance`: The average yearly balance in euros.\n",
        "7. `housing`: Whether the customer has a housing loan.\n",
        "8. `loan`: Whether the customer has a personal loan.\n",
        "9. `contact`: The type of communication contact\n",
        "10. `day`: The last contact day of the month.\n",
        "11. `month`: The last contact month of the year.\n",
        "12. `duration`: The last contact duration in seconds.\n",
        "13. `campaign`: Number of contacts performed during this campaign.\n",
        "14. `pdays`: Number of days since the client was last contacted.\n",
        "15. `previous`: Number of contacts before this campaign.\n",
        "16. `poutcome`: Outcome of the previous marketing campaign.\n",
        "17. `y`: Whether the customer subscribed to the term deposit (yes/no).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Load the data & view it\n"
      ],
      "metadata": {
        "id": "LqcJJXn1Fzpv"
      },
      "id": "LqcJJXn1Fzpv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hint: Use ; as the seperator."
      ],
      "metadata": {
        "id": "iLqa3tpuF9sK"
      },
      "id": "iLqa3tpuF9sK"
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the data\n",
        "# Read the csv file using a seperator \";\"\n",
        "\n",
        "\n",
        "# Viewing the data\n"
      ],
      "metadata": {
        "id": "PgfKmdEHElow"
      },
      "id": "PgfKmdEHElow",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 View the columns of the data and understand them"
      ],
      "metadata": {
        "id": "zAWBd6tVvfMj"
      },
      "id": "zAWBd6tVvfMj"
    },
    {
      "cell_type": "code",
      "source": [
        "# View the columns of the dataset\n"
      ],
      "metadata": {
        "id": "g2ugOQWXFoGZ"
      },
      "id": "g2ugOQWXFoGZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 Check the shape of the dataset"
      ],
      "metadata": {
        "id": "Q6dRlFFVvsDA"
      },
      "id": "Q6dRlFFVvsDA"
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the shape of the data\n"
      ],
      "metadata": {
        "id": "6V7JM4GgF1OH"
      },
      "id": "6V7JM4GgF1OH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4 Understand the datatypes of each of the columns"
      ],
      "metadata": {
        "id": "gdN7e6FVw0s7"
      },
      "id": "gdN7e6FVw0s7"
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the dataset info\n"
      ],
      "metadata": {
        "id": "8sFOJH8TGLtl"
      },
      "id": "8sFOJH8TGLtl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.5 Get a summary of the dataset using the describe method."
      ],
      "metadata": {
        "id": "p92AdEjQMFna"
      },
      "id": "p92AdEjQMFna"
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a statistical summary of the dataset\n"
      ],
      "metadata": {
        "id": "IdwtJKjuMI_a"
      },
      "id": "IdwtJKjuMI_a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 3: EDA\n"
      ],
      "metadata": {
        "id": "So2BjrTCPz66"
      },
      "id": "So2BjrTCPz66"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 3.1  Check the value counts of the target variable y in the dataset"
      ],
      "metadata": {
        "id": "8-0J_cZbMm_B"
      },
      "id": "8-0J_cZbMm_B"
    },
    {
      "cell_type": "markdown",
      "source": [
        "A common problem in binary classification tasks is data imbalance, where one class significantly outnumbers the other. In this question, you need to check whether the target variable y (whether the customer subscribed to a term deposit) is imbalanced in the dataset."
      ],
      "metadata": {
        "id": "UiImxZ4SMheQ"
      },
      "id": "UiImxZ4SMheQ"
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the value counts in target column\n"
      ],
      "metadata": {
        "id": "TXK9FW9WM17T"
      },
      "id": "TXK9FW9WM17T",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Plot the distribution of the value counts for the target variable y"
      ],
      "metadata": {
        "id": "D_2si9jiNUzT"
      },
      "id": "D_2si9jiNUzT"
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the value counts of 'y'\n",
        "\n",
        "\n",
        "# Plot the distribution\n"
      ],
      "metadata": {
        "id": "RFIzBb44NeCi"
      },
      "id": "RFIzBb44NeCi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 Plot a pie chart to visualize the distribution of the target variable y, including the percentages of each category"
      ],
      "metadata": {
        "id": "g2RJC3ktOhKb"
      },
      "id": "g2RJC3ktOhKb"
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the value counts of 'y'\n",
        "\n",
        "# Create a pie chart\n"
      ],
      "metadata": {
        "id": "uVKL0f4xOnfY"
      },
      "id": "uVKL0f4xOnfY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4 Why do you think classs imabalnce is a problem, and list down your ideas on addressing the class imbalance?"
      ],
      "metadata": {
        "id": "RP-dZasGPCdg"
      },
      "id": "RP-dZasGPCdg"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.5 For this problem what do you think is a good performance metric, precision, recall, f1-score or all of them? justify your answer."
      ],
      "metadata": {
        "id": "ll4NylrLPTnj"
      },
      "id": "ll4NylrLPTnj"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4:  Data Preprocessing"
      ],
      "metadata": {
        "id": "2HX8OWN-PrE8"
      },
      "id": "2HX8OWN-PrE8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Check for any duplicate rows in the dataset."
      ],
      "metadata": {
        "id": "DgegmF9EP_yz"
      },
      "id": "DgegmF9EP_yz"
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for duplicate row\n"
      ],
      "metadata": {
        "id": "Y5nezbLVO2r8"
      },
      "id": "Y5nezbLVO2r8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 Check the Null values\n"
      ],
      "metadata": {
        "id": "Rkptv-nNVc-M"
      },
      "id": "Rkptv-nNVc-M"
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values in the dataset\n"
      ],
      "metadata": {
        "id": "X57s62ohJ8OJ"
      },
      "execution_count": null,
      "outputs": [],
      "id": "X57s62ohJ8OJ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3 Split the data into train and test"
      ],
      "metadata": {
        "id": "4xmcOnxTRBse"
      },
      "id": "4xmcOnxTRBse"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f911619",
      "metadata": {
        "id": "5f911619"
      },
      "outputs": [],
      "source": [
        "# Assuming X and y are your features and target variable\n",
        "# Split the data into 80% train and 20% test, stratifying by the target variable (y)\n",
        "X =\n",
        "y =\n",
        "\n",
        "X_train, X_test, y_train, y_test =\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.4 Seperate the numerical and catergorical columns for both train and test sets"
      ],
      "metadata": {
        "id": "K1T7Ifj5TYv4"
      },
      "id": "K1T7Ifj5TYv4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4253ac44",
      "metadata": {
        "id": "4253ac44"
      },
      "outputs": [],
      "source": [
        "# Divide the dataset into numerical and categorical columns\n",
        "numerical_columns =\n",
        "categorical_columns =\n",
        "\n",
        "# Separate numerical and categorical columns for train and test data\n",
        "train_numerical =\n",
        "test_numerical =\n",
        "\n",
        "train_categorical =\n",
        "test_categorical =\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.5 Standardize the numerical columns using StandardScaler()"
      ],
      "metadata": {
        "id": "165JmBMxTFMD"
      },
      "id": "165JmBMxTFMD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5fc3181",
      "metadata": {
        "id": "e5fc3181"
      },
      "outputs": [],
      "source": [
        "# Apply StandardScaler to numerical columns in train and test data\n",
        "\n",
        "\n",
        "# Apply StandardScaler on train data (fit and transform)\n",
        "\n",
        "# Apply the scaler on test data (transform only)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.6 Encode the categorical columns using OneHotEncoder"
      ],
      "metadata": {
        "id": "cQj9s0o6Ue8r"
      },
      "id": "cQj9s0o6Ue8r"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3a2441c",
      "metadata": {
        "id": "d3a2441c"
      },
      "outputs": [],
      "source": [
        "# Import OneHotEncoder\n",
        "\n",
        "\n",
        "# Apply OneHotEncoder on train data (fit and transform)\n",
        "\n",
        "\n",
        "# Apply the encoder on test data (transform only)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.7 Combine the both numerical and categorical columns"
      ],
      "metadata": {
        "id": "qgE24GQYVCuY"
      },
      "id": "qgE24GQYVCuY"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5e60353",
      "metadata": {
        "id": "f5e60353"
      },
      "outputs": [],
      "source": [
        "# Combine numerical and categorical columns in train and test data\n",
        "\n",
        "# Convert the final train and test sets to DataFrames\n",
        "\n",
        "\n",
        "# Display the first few rows of the final processed training set\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.8 Encode the target variable (`yes: 1` , `no: 0`)\n"
      ],
      "metadata": {
        "id": "2R4NBmmpVf_0"
      },
      "id": "2R4NBmmpVf_0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fd84d44",
      "metadata": {
        "id": "1fd84d44"
      },
      "outputs": [],
      "source": [
        "# Encode the target variable in the training set\n",
        "\n",
        "\n",
        "# Encode the target variable in the test set\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.9 Display the shape of the train sets and test sets"
      ],
      "metadata": {
        "id": "x8VpRrkIWFs6"
      },
      "id": "x8VpRrkIWFs6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac0d558e",
      "metadata": {
        "id": "ac0d558e"
      },
      "outputs": [],
      "source": [
        "# Display the shape of sets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Model Building"
      ],
      "metadata": {
        "id": "fhDDrWGdWT8h"
      },
      "id": "fhDDrWGdWT8h"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1 Build a Logistic Regression Model"
      ],
      "metadata": {
        "id": "SIVMhUUVWdBD"
      },
      "id": "SIVMhUUVWdBD"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build a Logistic regression model and check the model performance using precision, recall, F1 score, and the AUC from ROC curve with default threshold. Write your observations based on the result."
      ],
      "metadata": {
        "id": "8gisx57pWg6W"
      },
      "id": "8gisx57pWg6W"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "355329ba",
      "metadata": {
        "id": "355329ba"
      },
      "outputs": [],
      "source": [
        "# Import LogisticRegression, required metrics\n",
        "\n",
        "\n",
        "# Step 1: Train a Logistic Regression model\n",
        "\n",
        "\n",
        "# Step 2: Predict probabilities for the test set (to calculate AUC later)\n",
        "\n",
        "\n",
        "# Step 3: Make predictions using the default threshold (0.5)\n",
        "\n",
        "\n",
        "# Step 4: Calculate Precision, Recall, and F1 Score\n",
        "\n",
        "\n",
        "# Step 5: Calculate AUC (Area Under the ROC Curve)\n",
        "\n",
        "\n",
        "# Step 6: Print the evaluation metrics\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15b2238a",
      "metadata": {
        "id": "15b2238a"
      },
      "source": [
        "### 5.2 Build a Logistic Regression Model with Optimal Threshold\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build a logisitc model with optimal threshold and check perfomrance based on above-mentioned metrics. Mention your observations on the recall and F1-Score along with other metrics(use `class_weight` = 'balanced')"
      ],
      "metadata": {
        "id": "vPXSwWAOW_4j"
      },
      "id": "vPXSwWAOW_4j"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b45eda7",
      "metadata": {
        "id": "7b45eda7"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Step 1: Train a Logistic Regression model\n",
        "\n",
        "\n",
        "# Step 2: Predict probabilities for the test set (to calculate AUC and ROC curve)\n",
        "\n",
        "\n",
        "# Step 3: Compute ROC curve and find the optimal threshold\n",
        "\n",
        "\n",
        "# Optimal threshold: the one that maximizes the difference between true positive rate and false positive rate\n",
        "\n",
        "\n",
        "# Step 4: Make predictions using the optimal threshold\n",
        "\n",
        "\n",
        "# Step 5: Calculate Precision, Recall, and F1 Score with the optimal threshold\n",
        "\n",
        "\n",
        "# Step 6: Calculate AUC (Area Under the ROC Curve)\n",
        "\n",
        "\n",
        "# Step 7: Print the evaluation metrics\n",
        "\n",
        "\n",
        "#Step 8: Plot\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fee8d108",
      "metadata": {
        "id": "fee8d108"
      },
      "source": [
        "### 5.3 Build a KNN Model with k=5\n",
        "\n",
        "Build a KNN model with k=5 and check the model performance using precision, recall, F1 score, write your observations on the metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39d8e7b3",
      "metadata": {
        "id": "39d8e7b3"
      },
      "outputs": [],
      "source": [
        "# Import KNeighborsClassifier\n",
        "\n",
        "\n",
        "# Make the train and test sets are in array format\n",
        "\n",
        "\n",
        "# Step 1: Initialize and train the KNN model\n",
        "\n",
        "\n",
        "# Step 2: Predict probabilities for the test set (to calculate AUC later)\n",
        "\n",
        "# Step 3: Make predictions using the default threshold (0.5)\n",
        "\n",
        "\n",
        "# Step 4: Calculate Precision, Recall, and F1 Score\n",
        "\n",
        "\n",
        "# Step 6: Print the evaluation metrics\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a00ca05",
      "metadata": {
        "id": "5a00ca05"
      },
      "source": [
        "### 5.4 Build a KNN Model with k=5 with a balanced set\n",
        "\n",
        "#####  SMOTE the data to address class imbalance and build a KNN model with k=5 and check the model performance using precision, recall, F1 score; Compare the reuslts with the previous results and write your observations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "658445eb",
      "metadata": {
        "id": "658445eb"
      },
      "outputs": [],
      "source": [
        "#Import modules\n",
        "\n",
        "# Step 1: Apply SMOTE to the training data\n",
        "\n",
        "\n",
        "# Resample X_train and y_train\n",
        "\n",
        "# Convert the resampled data to NumPy arrays (if they are not already)\n",
        "\n",
        "\n",
        "# Step 2: Initialize and train the KNN model\n",
        "\n",
        "# Step 3: Make predictions on the test set\n",
        "\n",
        "# Step 4: Calculate Precision, Recall, and F1 Score\n",
        "\n",
        "# Step 5: Print the evaluation metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5522c68",
      "metadata": {
        "id": "f5522c68"
      },
      "source": [
        "### 5.5 Build a Random forest model with n_estimators=10 and balanced class weight\n",
        "\n",
        "#####  Build a Random forest with the given specifications and check the model performance using precision, recall, F1 score; Obtain the training set and test set metrics and write your observations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78162e75",
      "metadata": {
        "id": "78162e75"
      },
      "outputs": [],
      "source": [
        "# Import required modules\n",
        "\n",
        "# Step 1: Initialize and train the Random Forest model with n_estimators=10\n",
        "\n",
        "# Step 2: Make predictions on the test set\n",
        "\n",
        "# Step 3: Make predictions on the training set\n",
        "\n",
        "\n",
        "# Step 4: Calculate Precision, Recall, and F1 Score for the test set\n",
        "\n",
        "\n",
        "# Step 5: Calculate Precision, Recall, and F1 Score for the training set\n",
        "\n",
        "\n",
        "# Step 6: Print the evaluation metrics for the training set\n",
        "\n",
        "# Step 7: Print the evaluation metrics for the test set\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35977c89",
      "metadata": {
        "id": "35977c89"
      },
      "source": [
        "### 5.6 Build a Random forest model with the following parameter grid and balanced class weight\n",
        "##### param_grid:\n",
        "\n",
        "```\n",
        "'n_estimators': [100,200],          \n",
        "'max_depth': [10,20,30],        \n",
        "'min_samples_split': [2,5,10]\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "#####  Build a Random forest with the given specifications and check the model performance using precision, recall, F1 score; Obtain the test set metrics and compare it with the previous model results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f40bd74",
      "metadata": {
        "id": "5f40bd74"
      },
      "outputs": [],
      "source": [
        "# Import required modules\n",
        "\n",
        "# Step 1: Define the hyperparameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [100,200],          # Number of trees\n",
        "    'max_depth': [10, 20, 30],        # Maximum depth of each tree\n",
        "    'min_samples_split': [2, 5, 10]         # Minimum samples required to split a node\n",
        "}\n",
        "\n",
        "# Step 2: Initialize the Random Forest model\n",
        "rf_model =\n",
        "\n",
        "# Step 3: Set up GridSearchCV\n",
        "grid_search =\n",
        "\n",
        "# Step 4: Fit the GridSearchCV on the training data\n",
        "\n",
        "\n",
        "# Step 5: Get the best parameters and best estimator\n",
        "\n",
        "\n",
        "# Step 6: Make predictions on the test set using the best model\n",
        "\n",
        "\n",
        "# Step 7: Calculate Precision, Recall, and F1 Score for the best model\n",
        "\n",
        "\n",
        "\n",
        "# Step 8: Print the evaluation metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6561a09d",
      "metadata": {
        "id": "6561a09d"
      },
      "source": [
        "### 5.7 Build a XGBoost model\n",
        "\n",
        "#####  Build a default XGBoost model and check the model performance using precision, recall, F1 score for training and test sets; Give your comments ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45ccf697",
      "metadata": {
        "id": "45ccf697"
      },
      "outputs": [],
      "source": [
        "# Import required modules\n",
        "\n",
        "# Step 1: Initialize and train the XGBoost model\n",
        "xgb_model =\n",
        "\n",
        "# Step 2: Train the model\n",
        "\n",
        "\n",
        "# Step 3: Make predictions on the test set\n",
        "\n",
        "\n",
        "# Step 4: Make predictions on the training set\n",
        "\n",
        "\n",
        "# Step 5: Calculate Precision, Recall, and F1 Score for the test set\n",
        "\n",
        "\n",
        "# Step 6: Calculate Precision, Recall, and F1 Score for the training set\n",
        "\n",
        "\n",
        "# Step 7: Print the evaluation metrics for the training set\n",
        "\n",
        "\n",
        "# Step 8: Print the evaluation metrics for the test set\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af752029",
      "metadata": {
        "id": "af752029"
      },
      "source": [
        "### 5.8 XGBoost model with hyper parameter tuning\n",
        "\n",
        "#####  Build a XGBoost model with the following parameter grid and check the model performance using precision, recall, F1 score on test set; Compare your result with the default XGBoost model .\n",
        "\n",
        "#### Hyperparameter grid:\n",
        "    - 'n_estimators': [50, 100, 200, 300],                 \n",
        "    - 'learning_rate': [0.01, 0.1, 0.2, 0.3],              \n",
        "    - 'max_depth': [None, 3, 5, 7, 10],                          \n",
        "    - 'min_child_weight': [1, 3, 5],                      \n",
        "    - 'subsample': [0.2, 0.6, 0.8, 1.0],                        \n",
        "    - 'colsample_bytree': [0.6, 0.8, 1.0],                 \n",
        "    - 'gamma': [0, 0.1, 0.3, 0.5]                          "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7cc3164c",
      "metadata": {
        "id": "7cc3164c"
      },
      "outputs": [],
      "source": [
        "# Import required modules\n",
        "\n",
        "# Step 1: Define the hyperparameter grid\n",
        "param_dist = {\n",
        "    'n_estimators': [50, 100, 200, 300],                 # Number of trees\n",
        "    'learning_rate': [0.01, 0.1, 0.2, 0.3],              # Learning rate\n",
        "    'max_depth': [None, 3, 5, 7, 10],                          # Maximum depth of each tree\n",
        "    'min_child_weight': [1, 3, 5],                       # Minimum sum of instance weight (hessian) needed in a child\n",
        "    'subsample': [0.2, 0.6, 0.8, 1.0],                        # Subsample ratio of the training instance\n",
        "    'colsample_bytree': [0.6, 0.8, 1.0],                 # Subsample ratio of columns when constructing each tree\n",
        "    'gamma': [0, 0.1, 0.3, 0.5]                          # Minimum loss reduction required to make a further partition on a leaf node\n",
        "}\n",
        "\n",
        "neg_count = (y_train == 0).sum()\n",
        "pos_count = (y_train == 1).sum()\n",
        "scale_pos_weight = neg_count / pos_count\n",
        "\n",
        "# Step 2: Initialize the XGBoost model\n",
        "xgb_model =\n",
        "\n",
        "# Step 3: Set up the RandomizedSearchCV\n",
        "random_search =\n",
        "\n",
        "\n",
        "# Step 4: Fit the RandomizedSearchCV on the training data\n",
        "\n",
        "\n",
        "# Step 5: Get the best parameters and best estimator\n",
        "\n",
        "\n",
        "\n",
        "# Step 6: Make predictions on the test set using the best model\n",
        "\n",
        "\n",
        "# Step 7: Calculate Precision, Recall, and F1 Score for the best model\n",
        "\n",
        "\n",
        "# Step 8: Print the evaluation metrics\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "827c2aad",
      "metadata": {
        "id": "827c2aad"
      },
      "source": [
        "### 5.9 XGBoost model with hyper parameter tuning and SMOTE\n",
        "\n",
        "#####  Build a XGBoost model with the parameter grid given in the previous problem and also balance the dataset with SMOTE and check the model performance using precision, recall, F1 score on test set; Compare your result with previous XGboost models ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9bbb2ae",
      "metadata": {
        "id": "e9bbb2ae"
      },
      "outputs": [],
      "source": [
        "# Import required modules\n",
        "\n",
        "# Step 1: Apply SMOTE to the training data to handle class imbalance\n",
        "\n",
        "\n",
        "# Step 2: Define the hyperparameter grid for XGBoost\n",
        "param_dist = {\n",
        "    'n_estimators': [50, 100, 200, 300],                 # Number of trees\n",
        "    'learning_rate': [0.01, 0.1, 0.2, 0.3],              # Learning rate\n",
        "    'max_depth': [3, 5, 7, 10],                          # Maximum depth of each tree\n",
        "    'min_child_weight': [1, 3, 5],                       # Minimum sum of instance weight (hessian) needed in a child\n",
        "    'subsample': [0.6, 0.8, 1.0],                        # Subsample ratio of the training instance\n",
        "    'colsample_bytree': [0.2, 0.6, 0.8, 1.0],                 # Subsample ratio of columns when constructing each tree\n",
        "    'gamma': [0, 0.1, 0.3, 0.5]                          # Minimum loss reduction required to make a further partition on a leaf node\n",
        "}\n",
        "\n",
        "# Step 3: Initialize the XGBoost model\n",
        "\n",
        "# Step 4: Set up RandomizedSearchCV\n",
        "\n",
        "# Step 5: Fit RandomizedSearchCV on the SMOTEd training data\n",
        "\n",
        "# Step 6: Get the best parameters and best estimator\n",
        "\n",
        "\n",
        "\n",
        "# Step 7: Make predictions on the test set using the best model\n",
        "\n",
        "\n",
        "# Step 8: Calculate Precision, Recall, and F1 Score for the best model\n",
        "\n",
        "\n",
        "# Step 9: Print the evaluation metrics\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a46e0f2",
      "metadata": {
        "id": "1a46e0f2"
      },
      "source": [
        "### 5.10 Best model selection\n",
        "\n",
        "#####  Evaluate the test set performance of all 9 models and create horizontal bar plots for each individual metric to visualize the results effectively. Based on the plots and your analysis, select the top three models and provide your reasoning for the selection.."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a805162d",
      "metadata": {
        "id": "a805162d"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Model names and their corresponding metrics (precision, recall, F1 score)\n",
        "model_names = [\n",
        "    \"Default Logistic Regression\", \"Logistic Regression with Optimal Threshold\",\n",
        "    \"KNN (k=5)\", \"KNN (k=5) + SMOTE\", \"Random Forest (10 Trees)\",\n",
        "    \"Tuned Random Forest\", \"Default XGBoost\", \"Tuned XGBoost\",\n",
        "    \"Tuned XGBoost + SMOTE\"\n",
        "]\n",
        "\n",
        "precision_scores =\n",
        "recall_scores =\n",
        "f1_scores =\n",
        "\n",
        "# Creating a DataFrame for easier manipulation\n",
        "df = pd.DataFrame({\n",
        "    'Model': model_names,\n",
        "    'Precision': precision_scores,\n",
        "    'Recall': recall_scores,\n",
        "    'F1-Score': f1_scores\n",
        "})\n",
        "\n",
        "# Sorting the DataFrame for each metric\n",
        "df_precision_sorted = df.sort_values(by='Precision', ascending=False)\n",
        "df_recall_sorted = df.sort_values(by='Recall', ascending=False)\n",
        "df_f1_sorted = df.sort_values(by='F1-Score', ascending=False)\n",
        "\n",
        "# Set style for plots\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Plot for Precision\n",
        "\n",
        "\n",
        "# Plot for Recall\n",
        "\n",
        "\n",
        "# Plot for F1-Score\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}